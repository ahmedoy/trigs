{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT4uYg1S0uAL"
      },
      "source": [
        "# Clone Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LdplNARo738",
        "outputId": "eecb251c-965e-4391-ad08-2ffb1a57fc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHRxeB-aw7Kh",
        "outputId": "26d0ab52-ae2b-4e7e-d8af-9b2fc0127673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'trigs'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 57 (delta 17), reused 44 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (57/57), 36.17 KiB | 9.04 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ahmedoy/trigs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9YrndgD0wVy"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e_0z7xryQj8",
        "outputId": "42ce55a1-f467-49c6-a65b-5cb9a38a2a60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vhhsUR6HPttYvLgTWN1_zE2RfI6BXvok\n",
            "From (redirected): https://drive.google.com/uc?id=1vhhsUR6HPttYvLgTWN1_zE2RfI6BXvok&confirm=t&uuid=945d12fa-19b7-45e5-a4f9-818d66d11963\n",
            "To: /content/drive/MyDrive/datasets/cifar/poison_train_cifar.zip\n",
            "100%|██████████| 1.84G/1.84G [00:49<00:00, 37.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VbctgwsMfMLFJwSLeVqykKnoBQ7Mvi_B\n",
            "From (redirected): https://drive.google.com/uc?id=1VbctgwsMfMLFJwSLeVqykKnoBQ7Mvi_B&confirm=t&uuid=5d47ea26-6ca7-46bf-a496-6febcab392dc\n",
            "To: /content/drive/MyDrive/datasets/cifar/clean_train_cifar.zip\n",
            "100%|██████████| 1.86G/1.86G [00:38<00:00, 48.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets unzipped and saved in Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "base_url = 'https://drive.google.com/uc?id='\n",
        "clean_train_cifar = base_url + '1VbctgwsMfMLFJwSLeVqykKnoBQ7Mvi_B'\n",
        "poison_train_cifar = base_url + '1vhhsUR6HPttYvLgTWN1_zE2RfI6BXvok'\n",
        "save_dir = '/content/drive/MyDrive/datasets/cifar/'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Step 2: Download the datasets to Google Drive\n",
        "poison_train_zip = save_dir + 'poison_train_cifar.zip'\n",
        "clean_train_zip = save_dir + 'clean_train_cifar.zip'\n",
        "# Download the zip files\n",
        "gdown.download(poison_train_cifar, poison_train_zip, quiet=False)\n",
        "gdown.download(clean_train_cifar, clean_train_zip, quiet=False)\n",
        "\n",
        "# Step 3: Unzip the datasets in Google Drive\n",
        "with zipfile.ZipFile(poison_train_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(save_dir + 'poison_train_cifar/')\n",
        "\n",
        "with zipfile.ZipFile(clean_train_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(save_dir + 'clean_train_cifar/')\n",
        "\n",
        "print(\"Datasets unzipped and saved in Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEyd3g1c3fyj"
      },
      "source": [
        "# Setup Library Versions\n",
        "\n",
        "Not required so far. Colab's libraries seem to work by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mKDMvTO3d8c"
      },
      "source": [
        "# Check Help Method in the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51Foqu9U38QN",
        "outputId": "0eaa3fd3-6547-4062-ff6a-b5d3c3cbc5ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: generate_model_signature.py [-h] --dataset_name DATASET_NAME [--model_name MODEL_NAME]\n",
            "                                   [--weights_path WEIGHTS_PATH] [--iterations ITERATIONS]\n",
            "                                   --learning_rate LEARNING_RATE --output_dir OUTPUT_DIR\n",
            "                                   [--opt_type {ADAM,SGD}] [--blur_freq BLUR_FREQ]\n",
            "                                   [--blur_sigma BLUR_SIGMA] [--blur_hks BLUR_HKS]\n",
            "                                   [--clipping_val CLIPPING_VAL] [--normalize_grad]\n",
            "                                   [--loss_type {logit,ce}] [--standardize_output]\n",
            "                                   [--clamp_pixels_freq CLAMP_PIXELS_FREQ] [--lambda_tv LAMBDA_TV]\n",
            "                                   [--batch_size BATCH_SIZE] [--debug]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataset_name DATASET_NAME\n",
            "                        Name of dataset used to train the probe models. Use Cifar10Ext for CIFAR10\n",
            "                        and TINExt for Tiny ImageNet\n",
            "  --model_name MODEL_NAME\n",
            "                        Name of the architecture for the probe model. Use vggmod with CIFAR10,\n",
            "                        resnet18_mod with Tiny ImageNet, and vit16b with ImageNet.\n",
            "  --weights_path WEIGHTS_PATH\n",
            "                        Path to model weights file.\n",
            "  --iterations ITERATIONS\n",
            "                        Number of optimization iterations. Use default to reproduce paper results.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        Learning rate. Use 10 with CIFAR10 and 0.1 with others to reproduce paper\n",
            "                        results.\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Path to store the resulting signature images.\n",
            "  --opt_type {ADAM,SGD}\n",
            "                        Optimizer to use. Use default to reproduce paper results.\n",
            "  --blur_freq BLUR_FREQ\n",
            "                        Frequency of applying Gaussian bluring. Use default to reproduce paper\n",
            "                        results.\n",
            "  --blur_sigma BLUR_SIGMA\n",
            "                        Sigma for the Gaussian blur. Use default to reproduce paper results.\n",
            "  --blur_hks BLUR_HKS   Half kernel width for the Gaussian blur. Use default to reproduce paper\n",
            "                        results.\n",
            "  --clipping_val CLIPPING_VAL\n",
            "                        Value beyond which to clip normalized gradient. 0 means no clipping. Use\n",
            "                        default to reproduce paper results.\n",
            "  --normalize_grad      If used, gradients will be normalized to a unit vector before every opt\n",
            "                        steps. Do NOT use to reproduce paper results.\n",
            "  --loss_type {logit,ce}\n",
            "                        Whether to optimize the logit value or the cross entropy loss. Use default\n",
            "                        to reproduce paper results.\n",
            "  --standardize_output  Convert each signature image to have 0.5 mean and 0.25 std. Use only with\n",
            "                        CIFAR10.\n",
            "  --clamp_pixels_freq CLAMP_PIXELS_FREQ\n",
            "                        Frequency of clamping pixel values to valid range. Use default to\n",
            "                        reproduce paper results.\n",
            "  --lambda_tv LAMBDA_TV\n",
            "                        Weight of the TV loss. Use 0.01 with Tiny ImageNet and 0.001 with others.\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Number of signature images to create at the same time. Adjust based on\n",
            "                        your GPU memory for ImageNet and use default for others.\n",
            "  --debug               Debug mode. Do NOT use to reproduce paper results.\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH=/content/trigs python /content/trigs/trigs/generate_model_signature.py -h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL-65612CF_a"
      },
      "source": [
        "# Get Clean and Poisoned Signatures\n",
        "\n",
        "\n",
        "Note that we have modified the original TRIGS repo to make this work\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyAi0MSeGOV1",
        "outputId": "ed1eef13-759c-48d0-ee1c-223e55976c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command executed successfully for 0000: SUCCESS!\n",
            "\n",
            "Command executed successfully for 0001: SUCCESS!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Function to create a placeholder weights file if it does not exist\n",
        "def create_placeholder_file(file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(\"\")  # Create an empty file\n",
        "\n",
        "for i in range(2):  # Loop from 0 to 499\n",
        "    num_str = f\"{i:04d}\"  # Format the number to four digits\n",
        "\n",
        "    output_dir = f\"/content/drive/MyDrive/datasets/cifar/signatures/clean/{num_str}\"\n",
        "    weights_path = f\"/content/drive/MyDrive/datasets/cifar/clean_train_cifar/clean_vggmod_CIFAR-10_{num_str}.pt\"\n",
        "\n",
        "    # Check if the weights file exists\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Creating placeholder weights file: {weights_path}\")\n",
        "        create_placeholder_file(weights_path)\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Construct the command\n",
        "    command = [\n",
        "        \"python3\", \"/content/trigs/trigs/generate_model_signature.py\",\n",
        "        \"--dataset_name\", \"Cifar10Ext\",\n",
        "        \"--model_name\", \"vggmod\",\n",
        "        \"--weights_path\", weights_path,\n",
        "        \"--iterations\", \"200\",\n",
        "        \"--learning_rate\", \"10\",\n",
        "        \"--output_dir\", output_dir,\n",
        "        \"--opt_type\", \"ADAM\",\n",
        "        \"--lambda_tv\", \"1e-3\",\n",
        "        \"--batch_size\", \"250\"\n",
        "    ]\n",
        "\n",
        "    # Execute the command and capture output\n",
        "    try:\n",
        "        result = subprocess.run(command, env={\"PYTHONPATH\": \"/content/trigs\"}, check=True, text=True, capture_output=True)\n",
        "        print(f\"Command executed successfully for {num_str}: {result.stdout}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error occurred for {num_str}: {e.stderr}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-frL7e7IdHO",
        "outputId": "ea823401-9674-4e5b-e63b-ca19ec3ecbd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command executed successfully for 0000: SUCCESS!\n",
            "\n",
            "Command executed successfully for 0001: SUCCESS!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(2):  # Loop from 0 to 499\n",
        "    num_str = f\"{i:04d}\"  # Format the number to four digits\n",
        "\n",
        "    output_dir = f\"/content/drive/MyDrive/datasets/cifar/signatures/poisoned/{num_str}\"\n",
        "    weights_path = f\"/content/drive/MyDrive/datasets/cifar/poison_train_cifar/poisoned_vggmod_CIFAR-10_{num_str}.pt\"\n",
        "\n",
        "    # Check if the weights file exists\n",
        "    if not os.path.exists(weights_path):\n",
        "        print(f\"Creating placeholder weights file: {weights_path}\")\n",
        "        create_placeholder_file(weights_path)\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Construct the command\n",
        "    command = [\n",
        "        \"python3\", \"/content/trigs/trigs/generate_model_signature.py\",\n",
        "        \"--dataset_name\", \"Cifar10Ext\",\n",
        "        \"--model_name\", \"vggmod\",\n",
        "        \"--weights_path\", weights_path,\n",
        "        \"--iterations\", \"200\",\n",
        "        \"--learning_rate\", \"10\",\n",
        "        \"--output_dir\", output_dir,\n",
        "        \"--opt_type\", \"ADAM\",\n",
        "        \"--lambda_tv\", \"1e-3\",\n",
        "        \"--batch_size\", \"250\"\n",
        "    ]\n",
        "\n",
        "    # Execute the command and capture output\n",
        "    try:\n",
        "        result = subprocess.run(command, env={\"PYTHONPATH\": \"/content/trigs\"}, check=True, text=True, capture_output=True)\n",
        "        print(f\"Command executed successfully for {num_str}: {result.stdout}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error occurred for {num_str}: {e.stderr}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
